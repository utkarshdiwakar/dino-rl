{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pyautogui\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pyautogui as inp\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebGame(Env):\n",
    "    def __init__(self):\n",
    "        self.restart_button_npy = np.load('restart_button.npy')\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=0, high=100, shape=(6,), dtype=np.uint8)\n",
    "        self.game_location = {'top': 275, 'left': 501, 'width': 343, 'height': 103}\n",
    "        self.retry_button = {'top':330, 'left':705, 'width':30, 'height':20}\n",
    "        self.new_dimensions = (60, 20)\n",
    "        self.cap = mss()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0:'space',\n",
    "            1:'down',\n",
    "            2:'no_op'\n",
    "        }\n",
    "        if action != 2:\n",
    "            inp.press(action_map[action])\n",
    "        \n",
    "        if_g_over = self.game_over()\n",
    "        new_obs = self.state()\n",
    "        reward = 10\n",
    "        if action==0: reward-=1\n",
    "        if action==1: reward-=1\n",
    "        \n",
    "        return new_obs, reward, if_g_over, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        inp.moveTo(290, 290)\n",
    "        inp.click()\n",
    "        inp.press('enter')\n",
    "        return self.state()\n",
    "    \n",
    "    def state(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        raw = np.mean(raw, axis=-1).astype(np.uint8)\n",
    "        _, raw = cv.threshold(raw, 200, 255, cv.THRESH_BINARY)\n",
    "        raw2 = raw.copy()\n",
    " \n",
    "        _, thresh = cv.threshold(raw2, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)\n",
    "        contours, hierarchy = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        min_contour_area = 250\n",
    "        filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
    "        border = 8\n",
    "        for contour in filtered_contours:\n",
    "            x, y, w, h = cv.boundingRect(contour)\n",
    "            cv.rectangle(raw2, (x- border, y - border), (x + w + border, y + h + border), (0, 0, 0), -1)\n",
    "\n",
    "        raw2 = cv.resize(raw2, self.new_dimensions, interpolation=cv.INTER_LANCZOS4)\n",
    "        _, raw2 = cv.threshold(raw2, 195, 255, cv.THRESH_BINARY)\n",
    "\n",
    "        contours, hierarchy = cv.findContours(cv.bitwise_not(raw2), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        min_contour_area = 1\n",
    "        filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
    "        filtered_contours = filtered_contours[::-1]\n",
    "\n",
    "        if (len(filtered_contours) < 1): \n",
    "            channel = np.array([100,100,100,100,100,100])\n",
    "    \n",
    "        elif (len(filtered_contours) == 1):\n",
    "            x, y, w, h = cv.boundingRect(filtered_contours[0])\n",
    "            channel = np.array([x,h,w,100,100,100])\n",
    "        else:\n",
    "            x1, y1, w1, h1 = cv.boundingRect(filtered_contours[0])\n",
    "            x2, y2, w2, h2 = cv.boundingRect(filtered_contours[1])\n",
    "            channel  = np.array([x1,h1,w1,x2,h2,w2])\n",
    "            \n",
    "        return channel\n",
    "    \n",
    "    \n",
    "    def game_over(self):\n",
    "        retry_img = np.array(self.cap.grab(self.retry_button))[:,:,:3]\n",
    "        if np.array_equal(retry_img, self.restart_button_npy): return True\n",
    "        else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WebGame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,  20,  60, 100, 100, 100]), 9, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  60 100 100 100]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(env.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, 6)))\n",
    "    model.add(Dense(64, activation='relu', input_shape=states))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                448       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12995 (50.76 KB)\n",
      "Trainable params: 12995 (50.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=500000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 12:56:07.980506: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-10-16 12:56:08.037031: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_5_1/kernel/Assign' id:257 op device:{requested: '', assigned: ''} def:{{{node dense_5_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_5_1/kernel, dense_5_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdditionalUpdatesOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdditionalUpdatesOptimizer`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdditionalUpdatesOptimizer`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    6/10000 [..............................] - ETA: 2:05 - reward: 10.0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarsh/miniconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-10-16 12:56:08.645430: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_7/BiasAdd' id:217 op device:{requested: '', assigned: ''} def:{{{node dense_7/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_7/MatMul, dense_7/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-10-16 12:56:08.668487: W tensorflow/c/c_api.cc:304] Operation '{name:'count/Assign' id:490 op device:{requested: '', assigned: ''} def:{{{node count/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count, count/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11/10000 [..............................] - ETA: 2:01 - reward: 10.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarsh/miniconda3/lib/python3.11/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "2023-10-16 12:56:08.863359: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_7_1/BiasAdd' id:324 op device:{requested: '', assigned: ''} def:{{{node dense_7_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_7_1/MatMul, dense_7_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get_updates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/utkarsh/Desktop/ml_programs/rl/ChromeDino/main.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/utkarsh/Desktop/ml_programs/rl/ChromeDino/main.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dqn \u001b[39m=\u001b[39m build_agent(model, actions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/utkarsh/Desktop/ml_programs/rl/ChromeDino/main.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/utkarsh/Desktop/ml_programs/rl/ChromeDino/main.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dqn\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m5000000\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/rl/core.py:193\u001b[0m, in \u001b[0;36mAgent.fit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m nb_max_episode_steps \u001b[39mand\u001b[39;00m episode_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m nb_max_episode_steps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    191\u001b[0m     \u001b[39m# Force a terminal state.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(reward, terminal\u001b[39m=\u001b[39;49mdone)\n\u001b[1;32m    194\u001b[0m episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m    196\u001b[0m step_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m    197\u001b[0m     \u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m: action,\n\u001b[1;32m    198\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m: observation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m: accumulated_info,\n\u001b[1;32m    203\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/rl/agents/dqn.py:321\u001b[0m, in \u001b[0;36mDQNAgent.backward\u001b[0;34m(self, reward, terminal)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39m# Finally, perform a single update on the entire batch. We use a dummy target since\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# the actual loss is computed in a Lambda layer that needs more complex input. However,\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# it is still useful to know the actual target to compute metrics properly.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m ins \u001b[39m=\u001b[39m [state0_batch] \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minput) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlist\u001b[39m \u001b[39melse\u001b[39;00m state0_batch\n\u001b[0;32m--> 321\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_model\u001b[39m.\u001b[39;49mtrain_on_batch(ins \u001b[39m+\u001b[39;49m [targets, masks], [dummy_targets, targets])\n\u001b[1;32m    322\u001b[0m metrics \u001b[39m=\u001b[39m [metric \u001b[39mfor\u001b[39;00m idx, metric \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(metrics) \u001b[39mif\u001b[39;00m idx \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)]  \u001b[39m# throw away individual losses\u001b[39;00m\n\u001b[1;32m    323\u001b[0m metrics \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:1180\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1177\u001b[0m         ins \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mTrue\u001b[39;00m]  \u001b[39m# Add learning phase value.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_sample_weight_modes(sample_weights\u001b[39m=\u001b[39msample_weights)\n\u001b[0;32m-> 1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_train_function()\n\u001b[1;32m   1181\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(ins)\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m reset_metrics:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2284\u001b[0m, in \u001b[0;36mModel._make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mget_graph()\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m   2282\u001b[0m     \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2283\u001b[0m         \u001b[39m# Training updates\u001b[39;00m\n\u001b[0;32m-> 2284\u001b[0m         updates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mget_updates(\n\u001b[1;32m   2285\u001b[0m             params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collected_trainable_weights,\n\u001b[1;32m   2286\u001b[0m             loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_loss,\n\u001b[1;32m   2287\u001b[0m         )\n\u001b[1;32m   2288\u001b[0m         \u001b[39m# Unconditional updates\u001b[39;00m\n\u001b[1;32m   2289\u001b[0m         updates \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_updates_for(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_updates'"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=5000000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
